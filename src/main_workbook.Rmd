---
title: "EC4308 Project (Lending Club)"
author: "Brandon, LX, WT, YH, ZH"
date: "2024-10-11"
output: html_document
---

## Initial setup
```{r setup, include=FALSE}
library(tidyverse)
library(jsonlite)
library(lubridate)
library(zoo)
library(rpart)
library(rpart.plot)
library(randomForest)
library(pROC)
library(caret)
library(glmnet)
library(xgboost)

data <- readRDS("../data/combined_data.rds")
```

## Training/Test Split
```{r}
# Set seed
set.seed(123)

n <- nrow(data)  

# Define the proportion for each set
train_ratio <- 0.6  # 60% for training
validation_ratio <- 0.2  # 20% for validation
test_ratio <- 0.2  # 20% for testing

train_indices <- sample(seq_len(n), size = train_ratio * n)
remaining_indices <- setdiff(seq_len(n), train_indices)

validation_indices <- sample(remaining_indices, size = validation_ratio * n)
test_indices <- setdiff(remaining_indices, validation_indices)

train_data <- data[train_indices, ]
validation_data <- data[validation_indices, ]
test_data <- data[test_indices, ]

# Replace NA values with 0 in both train and test datasets
train_data <- train_data %>% mutate(across(everything(), ~ ifelse(is.na(.), 0, .)))
test_data <- test_data %>% mutate(across(everything(), ~ ifelse(is.na(.), 0, .)))

#Check the sizes of each set
cat("Train size: ", nrow(train_data), "\n")
cat("Validation size: ", nrow(validation_data), "\n")
cat("Test size: ", nrow(test_data), "\n")

```

## Calculation metrics
Below are the functions used to calculate the metrics
```{r}
# Accuracy
calculate_accuracy <- function(confusion_matrix) {
  TP <- confusion_matrix[2, 2]
  TN <- confusion_matrix[1, 1]
  accuracy <- (TP + TN) / sum(confusion_matrix)
  return(accuracy)
}

# Recall (Sensitivity)
calculate_recall <- function(confusion_matrix) {
  TP <- confusion_matrix[2, 2]
  FN <- confusion_matrix[2, 1]
  recall <- TP / (TP + FN)
  return(recall)
}

# False Negative Rate (FNR)
calculate_fnr <- function(confusion_matrix) {
  FN <- confusion_matrix[2, 1]
  TP <- confusion_matrix[2, 2]
  fnr <- FN / (TP + FN)
  return(fnr)
}

# Precision
calculate_precision <- function(confusion_matrix) {
  TP <- confusion_matrix[2, 2]
  FP <- confusion_matrix[1, 2]
  precision <- TP / (TP + FP)
  return(precision)
}

# F1 Score
calculate_f1_score <- function(confusion_matrix) {
  recall <- calculate_recall(confusion_matrix)
  precision <- calculate_precision(confusion_matrix)
  f1_score <- 2 * (precision * recall) / (precision + recall)
  return(f1_score)
}

```

## Baseline Model - Logistic Regression
For refresher purposes, 0 is a good loan, 1 is a bad loan
```{r}
#Fitting logistic regression model using all variables
set.seed(42)
logit.fit = glm(will_default ~ ., data=train_data, family="binomial")

summary(logit.fit)

# Generate predictions
logit_predictions = round(predict(logit.fit, test_data, type='response'))
```
## Metrics
```{r}
logit_confusion_matrix <- table(test_data$will_default, logit_predictions)
print("Confusion Matrix:")
print(logit_confusion_matrix)

logit_accuracy <- calculate_accuracy(logit_confusion_matrix)
cat("Accuracy:", round(logit_accuracy, 4), "\n")

logit_fnr <- calculate_fnr(logit_confusion_matrix)
cat("False Negatives (FN):", logit_fnr, "\n")

logit_recall <- calculate_recall(logit_confusion_matrix)
cat("Recall:", round(logit_recall, 4), "\n")

logit_f1_score <- calculate_f1_score(logit_confusion_matrix)
cat("F1 Score", round(logit_f1_score, 4), "\n")
```



## Decision tree with depth 7
### Creating the tree
```{r}
decision_tree.fit <- rpart(will_default ~ ., 
                       data = train_data, 
                       method = "class", 
                       control = rpart.control(maxdepth = 7))

```

### Print the tree
```{r}
rpart.plot(decision_tree.fit)
```
### Predict on test data
```{r}
tree_predictions <- predict(decision_tree.fit, test_data, type = "prob")[,2]
```

### Confusion Matrix
```{r}
confusion_matrix <- table(test_data$will_default, tree_predictions)
print("Confusion Matrix:")
print(confusion_matrix)

accuracy <- calculate_accuracy(confusion_matrix)
cat("Accuracy:", round(accuracy, 4), "\n")

false_negative <- calculate_fnr(confusion_matrix)
cat("False Negatives (FN):", false_negative, "\n")
cat("False Negative Percentage:", round(false_negative, 2), "%\n")

recall <- calculate_recall(confusion_matrix)
cat("Recall:", round(recall, 4), "\n")

f1_score <- calculate_f1_score(confusion_matrix)
cat("F1 Score", round(f1_score, 4), "\n")
```
## Random Forest
```{r}

rf <- randomForest(will_default ~ ., 
                            data = train_data, 
                            ntree = 500,
                            mtry = sqrt(ncol(train_data) - 1),
                            importance = TRUE)

```

### Importance plot
```{r}
importance(rf)
varImpPlot(rf)
```
### Visualize class specific error rates by ntree
```{r}
ntree.error = data.frame(ntrees = rep(1:nrow(rf$err.rate), times = 3),
                       class = rep(c("OOB", "Did not default", "Default"), each = nrow(rf$err.rate)),
                       error = c(rf$err.rate[, "OOB"], rf$err.rate[, "0"], rf$err.rate[, "1"])
                        )
# OOB error rate refers to the average error rate calculated on OOB samples
# Each tree in the RF sees only a subset of data, hence the OOB samples serve as a validation set to estimate the model's error during training

ggplot(ntree.error, aes(x = ntrees, y = error, color = class)) +
  geom_line() +
  ggtitle("Training Error Rates")
```
We can see that the class specific error rate stabilizes at around 400 trees. The "Did not default" error rates lying above the OOB and "Default" error rates do seem to suggest that the model is relatively more accurate at predicting defaults than it is at correctly predicting the overall dataset, i.e. the RF model seems to struggle more at correctly identifying samples that did not default.

### Prediction
```{r}
# Predict on test data
rf_predictions <- predict(rf, test_data, type = "prob")[,2]
```

### Metrics
```{r}
confusion_matrix <- table(test_data$will_default, rf_predictions)
print("Confusion Matrix:")
print(confusion_matrix)

accuracy <- calculate_accuracy(confusion_matrix)
cat("Accuracy:", round(accuracy, 4), "\n")

false_negative <- calculate_fnr(confusion_matrix)
cat("False Negatives (FN):", false_negative, "\n")

recall <- calculate_recall(confusion_matrix)
cat("Recall:", round(recall, 4), "\n")

f1_score <- calculate_f1_score(confusion_matrix)
cat("F1 Score", round(f1_score, 4), "\n")

```

## xgboost

## LASSO Classifier
```{r}
# Perform 10 fold CV to determine lambda
x_train <- model.matrix(will_default ~ . - 1, data = train_data)
y_train <- train_data$will_default

x_test <- model.matrix(will_default ~ . - 1, data = test_data)
y_test <- test_data$will_default

cv_lasso <- cv.glmnet(x_train, y_train, family = "binomial", alpha = 1)
best_lambda_lasso <- cv_lasso$lambda.min

# Fit LASSO model using lambda found 
lasso_model <- glmnet(x_train, y_train, family = "binomial", alpha = 1, lambda = best_lambda_lasso)
```

### Prediction
```{r}
lasso_predictions <- predict(lasso_model, x_test, type = "class")
```

### Metrics
```{r}
lasso_confusion_matrix <- table(test_data$will_default, lasso_predictions)
cat("LASSO Confusion Matrix: \n")
print(lasso_confusion_matrix)

lasso_accuracy <- calculate_accuracy(lasso_confusion_matrix)
cat("LASSO Accuracy:", round(lasso_accuracy, 4), "\n")

lasso_fnr <- calculate_fnr(lasso_confusion_matrix)
cat("LASSO False Negative Rate:", round(lasso_fnr, 4), "\n")

lasso_f1_score <- calculate_f1_score(lasso_confusion_matrix)
cat("LASSO F1 Score:", round(lasso_f1_score, 4), "\n")
```

## Ridge Classifier
```{r}
# Perform 10 fold CV to determine lambda
cv_ridge <- cv.glmnet(x_train, y_train, family = "binomial", alpha = 0)
best_lambda_ridge <- cv_ridge$lambda.min

# Fit LASSO model using lambda found 
ridge_model <- glmnet(x_train, y_train, family = "binomial", alpha = 0, lambda = best_lambda_ridge)
```

###Prediction
```{r}
ridge_predictions <- predict(ridge_model, newx = x_test, type = "class")
```

### Metrics
```{r}
ridge_confusion_matrix <- table(test_data$will_default, ridge_predictions)
cat("Ridge Confusion Matrix:\n")
print(ridge_confusion_matrix)

ridge_accuracy <- calculate_accuracy(ridge_confusion_matrix)
cat("Ridge Accuracy:", round(ridge_accuracy, 4), "\n")

ridge_fnr <- calculate_fnr(ridge_confusion_matrix)
cat("Ridge False Negative Rate:", round(ridge_fnr, 4), "\n")

ridge_f1_score <- calculate_f1_score(ridge_confusion_matrix)
cat("Ridge F1 Score:", round(ridge_f1_score, 4), "\n")
```

## Final Comparison
### Recall Precision Plot
```{r}
plot_data <- data.frame(recall = numeric(), precision = numeric(), model = character())

# Function to calculate precision and recall
calculate_precision_recall <- function(probs, labels, model_name) {
  roc_obj <- roc(labels, probs)
  pr_data <- coords(roc_obj, x = "all", ret = c("recall", "precision"), transpose = FALSE)
  data.frame(Recall = pr_data$recall, Precision = pr_data$precision, Model = model_name)
}

# Model 1: Logistic Regression
logit_pr <- calculate_precision_recall(logit_predictions, test_data$will_default, "Logistic Regression")
plot_data <- rbind(plot_data, logit_pr)

# Model 2: Decision Tree
tree_pr <- calculate_precision_recall(tree_predictions, test_data$will_default, "Decision Tree")
plot_data <- rbind(plot_data, tree_pr)

# Model 3: Random Forest
rf_pr <- calculate_precision_recall(rf_predictions, test_data$will_default, "Random Forest")
plot_data <- rbind(plot_data, rf_pr)

# Model 4: LASSO
lasso_plot <- calculate_precision_recall(as.numeric(lasso_predictions), y_test, "LASSO")
plot_data <- rbind(plot_data, lasso_pr)

# Model 5: Ridge
ridge_plot <- calculate_precision_recall(as.numeric(ridge_predictions), y_test, "Ridge")
plot_data <- rbind(plot_data, ridge_pr)

ggplot(plot_data, aes(x = Recall, y = Precision, color = Model)) +
  geom_line() +
  labs(title = "Precision-Recall Curve for Various Models", x = "Recall", y = "Precision") +
  theme_minimal()
```

### Table of all results 
```{r}
# Initialize an empty data frame for storing results
results_table <- data.frame(
  Model = character(),
  Accuracy = numeric(),
  Recall = numeric(),
  Precision = numeric(),
  FNR = numeric(),
  F1_Score = numeric(),
  stringsAsFactors = FALSE
)

models <- list(
  list(name = "Decision Tree", predictions = predict(baseline_tree, test_data, type = "class")),
  list(name = "Random Forest", predictions = predict(rf, test_data, type = "class")),
  list(name = "LASSO", predictions = predict(lasso_model, x_test, type = "class")),
  list(name = "Ridge", predictions = predict(ridge_model, x_test, type = "class"))
)

# Loop through each model, calculate metrics, and add results to  table
for (model in models) {
  model_name <- model$name
  predictions <- as.factor(model$predictions) 
  actuals <- as.factor(test_data$will_default)
  
  confusion_matrix <- table(actuals, predictions)
  accuracy <- calculate_accuracy(confusion_matrix)
  recall <- calculate_recall(confusion_matrix)
  precision <- calculate_precision(confusion_matrix)
  fnr <- calculate_fnr(confusion_matrix)
  f1_score <- calculate_f1_score(confusion_matrix)
  
  results_table <- rbind(results_table, data.frame(
    Model = model_name,
    Accuracy = round(accuracy, 4),
    Recall = round(recall, 4),
    Precision = round(precision, 4),
    FNR = round(fnr, 4),
    F1_Score = round(f1_score, 4)
  ))
}

print(results_table)

```

