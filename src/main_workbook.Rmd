---
title: "EC4308 Project (Lending Club)"
author: "Brandon, LX, WT, YH, ZH"
date: "2024-10-11"
output: html_document
---

## Initial setup
```{r setup, include=FALSE}
library(readr)
library(tidyverse)
library(jsonlite)
library(lubridate)
library(zoo)
library(dplyr)
library(rpart)
library(rpart.plot)
library(randomForest)
library(pROC)

data <- readRDS("../data/combined_data.rds")
```

## Training/Test Split
```{r}
# Set seed
set.seed(123)

n <- nrow(data)  

# Define the proportion for each set
train_ratio <- 0.6  # 60% for training
validation_ratio <- 0.2  # 20% for validation
test_ratio <- 0.2  # 20% for testing

train_indices <- sample(seq_len(n), size = train_ratio * n)
remaining_indices <- setdiff(seq_len(n), train_indices)

validation_indices <- sample(remaining_indices, size = validation_ratio * n)
test_indices <- setdiff(remaining_indices, validation_indices)

train_data <- data[train_indices, ]
validation_data <- data[validation_indices, ]
test_data <- data[test_indices, ]

# Replace NA values with 0 in both train and test datasets
train_data <- train_data %>% mutate(across(everything(), ~ ifelse(is.na(.), 0, .)))
test_data <- test_data %>% mutate(across(everything(), ~ ifelse(is.na(.), 0, .)))

#Check the sizes of each set
cat("Train size: ", nrow(train_data), "\n")
cat("Validation size: ", nrow(validation_data), "\n")
cat("Test size: ", nrow(test_data), "\n")

```

## Calculation metrics
Below are the functions used to calculate the metrics
```{r}
# Accuracy
calculate_accuracy <- function(confusion_matrix) {
  TP <- confusion_matrix[2, 2]
  TN <- confusion_matrix[1, 1]
  accuracy <- (TP + TN) / sum(confusion_matrix)
  return(accuracy)
}

# Recall (Sensitivity)
calculate_recall <- function(confusion_matrix) {
  TP <- confusion_matrix[2, 2]
  FN <- confusion_matrix[2, 1]
  recall <- TP / (TP + FN)
  return(recall)
}

# False Negative Rate (FNR)
calculate_fnr <- function(confusion_matrix) {
  FN <- confusion_matrix[2, 1]
  TP <- confusion_matrix[2, 2]
  fnr <- FN / (TP + FN)
  return(fnr)
}

# Precision
calculate_precision <- function(confusion_matrix) {
  TP <- confusion_matrix[2, 2]
  FP <- confusion_matrix[1, 2]
  precision <- TP / (TP + FP)
  return(precision)
}

# F1 Score
calculate_f1_score <- function(confusion_matrix) {
  recall <- calculate_recall(confusion_matrix)
  precision <- calculate_precision(confusion_matrix)
  f1_score <- 2 * (precision * recall) / (precision + recall)
  return(f1_score)
}

```


## Baseline model - Decision tree with depth 7
For refresher purposes, 0 is a good loan, 1 is a bad loan
### Creating the tree
```{r}
baseline_tree <- rpart(will_default ~ ., 
                       data = train_data, 
                       method = "class", 
                       control = rpart.control(maxdepth = 7))

```


### Print the tree
```{r}
rpart.plot(baseline_tree)
```
### Predict on test data
```{r}
predictions <- predict(baseline_tree, test_data, type = "class")
```

### Confusion Matrix
```{r}
confusion_matrix <- table(test_data$will_default, predictions)
print("Confusion Matrix:")
print(confusion_matrix)

accuracy <- calculate_accuracy(confusion_matrix)
cat("Accuracy:", round(accuracy, 4), "\n")

false_negative <- calculate_fnr(confusion_matrix)
cat("False Negatives (FN):", false_negative, "\n")
cat("False Negative Percentage:", round(false_negative_percentage, 2), "%\n")

recall <- calculate_recall(confusion_matrix)
cat("True Positives (TP):", TP, "\n")
cat("Recall:", round(recall, 4), "\n")

f1_score <- calculate_f1_score(confusion_matrix)
cat("F1 Score", round(f1_score, 4), "\n")
```

This is an upper estimate, we are assuming that they did not even pay a single cent of money.

## ML Model 1: Random Forest
```{r}

rf <- randomForest(will_default ~ ., 
                            data = train_data, 
                            ntree = 500,
                            mtry = sqrt(ncol(train_data) - 1),
                            importance = TRUE)

```

### Importance plot
```{r}
# Predict on the test data
importance(rf)
varImpPlot(rf)

```
### Confusion Matrix
```{r}
# Confusion matrix
rf_predictions <- predict(rf, test_data, type = "class")
```

### Metrics
```{r}
confusion_matrix <- table(test_data$will_default, rf_predictions)
print("Confusion Matrix:")
print(confusion_matrix)

accuracy <- calculate_accuracy(confusion_matrix)
cat("Accuracy:", round(accuracy, 4), "\n")

false_negative <- calculate_fnr(confusion_matrix)
cat("False Negatives (FN):", false_negative, "\n")
cat("False Negative Percentage:", round(false_negative_percentage, 2), "%\n")

recall <- calculate_recall(confusion_matrix)
cat("True Positives (TP):", TP, "\n")
cat("Recall:", round(recall, 4), "\n")

f1_score <- calculate_f1_score(confusion_matrix)
cat("F1 Score", round(f1_score, 4), "\n")

```

