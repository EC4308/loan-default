---
title: "EC4308 Project (Lending Club)"
author: "Brandon, LX, WT, YH, ZH"
date: "2024-10-11"
output: html_document
---

## Initial setup
```{r setup, include=FALSE}
library(readr)
library(tidyverse)
library(jsonlite)
library(lubridate)
library(zoo)
```

### Convert all the csv files to RDS to save space
```{r}
# Step 1: Load the CSV files
#lc_loan <- read_csv("../data/lc_loan.csv")
#lc_2016_2017 <- read_csv("../data/lc_2016_2017.csv")

# Step 2: Save the loaded data as RDS files
#saveRDS(lc_loan, "../data/lc_loan.rds")
#saveRDS(lc_2016_2017, "../data/lc_2016_2017.rds")
```

## Read the RDS Files
```{r}
lc_loan <- readRDS("../data/lc_loan.rds")
lc_2016_2017 <- readRDS("../data/lc_2016_2017.rds")
```

### Analysing the head of the data
```{r cars}
head(lc_loan)
```

## Data Cleaning
### Observe the difference in columns
```{r}
# Get the column names from both dataframes
lc_loan_cols <- colnames(lc_loan)
lc_2016_2017_cols <- colnames(lc_2016_2017)

# Compare the columns to find the differences
diff_cols <- setdiff(lc_loan_cols, lc_2016_2017_cols)
diff_cols_2 <- setdiff(lc_2016_2017_cols, lc_loan_cols)

# Display the differences
cat("Columns in 'lc_loan' but not in 'lc_2016_2017':", diff_cols, "\n")
cat("Columns in 'lc_2016_2017' but not in 'lc_loan':", diff_cols_2, "\n")
```

### Remove columns that are not common between the two dataframes
```{r}
lc_loan_clean <- lc_loan %>% select(-all_of(diff_cols))
lc_2016_2017_clean <- lc_2016_2017 %>% select(-all_of(diff_cols_2))
combined_data <- bind_rows(lc_loan_clean, lc_2016_2017_clean)
glimpse(combined_data)
```

## WT + ZH
## Index loan status to 0 = Fully paid + Current, 1 = Late + Charged Off + Late  + Grace period + Default
### Convert loan_status to a factor (categorical variable)
```{r}
combined_data <- combined_data %>%
  mutate(loan_status = as.factor(loan_status))
table(combined_data$loan_status)
```

### Print out the possible levels of loan_status
```{r}
cat("Possible loan_status categories:\n")
print(levels(combined_data$loan_status))
```

### Remove "Current" as it adds no value to our analytics
```{r}
combined_data <- combined_data %>%
  filter(loan_status != "Current")
```

### Now convert it to binary 
```{r}
combined_data <- combined_data %>%
  mutate(
    loan_status_bin = case_when(
      loan_status %in% c("Fully Paid", "Does not meet the credit policy. Status:Fully Paid") ~ 0,
      TRUE ~ 1  # Everything else will be 1
    )
  )

head(combined_data)
```

### Check how many 0s and 1s in the new binary column
```{r}
loan_status_distribution <- table(combined_data$loan_status_bin)
print(loan_status_distribution)

paid_off_percentage <- (loan_status_distribution[1] / sum(loan_status_distribution)) * 100

cat("Percentage of loans that are paid off in the full data set:", round(paid_off_percentage, 2), "%\n")

```
# Brandon + LX
## FRED data on Delinquency Rate on Consumer Loans, All Commercial Banks (Last payment date + 3 months)
```{r}
combined_data %>%
  mutate(last_pymnt_d = if_else(is.na(last_pymnt_d), issue_d, last_pymnt_d),  # Replace NA with issue_d so that those that did not pay a single payment has a date to merge with the delinquency data 
         quarter = as.yearqtr(dmy(paste("01", last_pymnt_d, sep = "-")))) -> combined_data

delinquency_rate <- read_csv("../data/DRCLACBS.csv") 
delinquency_rate <- delinquency_rate %>%
  rename("delinquency_rate" = "DRCLACBS") %>%
  mutate(DATE = as.yearqtr(ymd(DATE)))

combined_data %>% 
  left_join(delinquency_rate, by = c("quarter" = "DATE")) -> combined_data
```

### Stratify the data based on the loan_status and get ~ 50,000 accordingly
Random suggestion, compare the direct pulling of 50k data points vs stratified
```{r}
## Stratify data based on loan status
# Set seed for reproducibility
set.seed(42)

# Stratified Sampling by 'loan status'
strat_cols <- c("loan_status")

# Perform the stratified sampling for training set (20%)
train_data <- combined_data %>%
  group_by(across(all_of(strat_cols))) %>%
  sample_frac(0.2) %>% # Adjust the fraction as needed for training set
  ungroup()

# # Remaining data for test sets
test_data <- anti_join(combined_data, train_data)
```

#### Random pulling
```{r}
# Set the seed for reproducibility
set.seed(4308)

# Randomly sample 50,000 rows from the dataset, as it is getting way too large
random_sampled_data <- combined_data %>%
  sample_n(size = 50000)

loan_status_distribution <- table(random_sampled_data$loan_status_bin)
print(loan_status_distribution)

paid_off_percentage <- (loan_status_distribution[1] / sum(loan_status_distribution)) * 100

cat("Percentage of loans that are paid off, when we do a random sample of 50,000:", round(paid_off_percentage, 2), "%\n")
```
Quite accurate as compared to the real world proportions. But by doing a random sample, we might not have enough readings for 1 as 30% is quite a small amount.

#### Stratified
Maybe we can do a 50/50 stratification to get a better sensing on real data?
```{r}
## Random 50/50 stratification 
set.seed(123) 

combined_data <- combined_data %>%
  mutate(group = sample(c(1, 2), n(), replace = TRUE))

group_1 <- combined_data %>% filter(group == 1)
group_2 <- combined_data %>% filter(group == 2)

# Double-check 50/50 split
nrow(group_1)
nrow(group_2)
```
1. Strat (50/50)
2. Random
Run these into models, and see which has lower classification error

### Label the data

# YH + Grab anyone for help
```{r recode-employment-length}
combined_data %>%
  ggplot(aes(x=emp_length)) +
  geom_bar(fill='steelblue') +
  theme_minimal() + 
  labs(x='Employment Length', y='Number of Loans', title='Distribution of Employment Length')

# Encode "< 1 year" and missing values as 0, "10+ years" as 10
combined_data = combined_data %>%
  mutate(emp_length = case_when(
    emp_length == '< 1 year' ~ 0,
    emp_length == 'n/a' ~ 0,
    is.na(emp_length) ~ 0,
    emp_length == '10+ years' ~ 10,
    TRUE ~ as.numeric(gsub(" years?$", "", emp_length))
  ))
```
We can see that most of the borrowers have been employed for more than 10 years. As a preliminary cleaning step, we can remove the trailing " year" or " years" in the category name, as well as encode the "< 1 year" or missing values as 0. After we conduct our EDA, we can consider binning the `emp_length` column, for example 0-3 years, 4-7 years etc.

```{r recode-verification-status}
# Combine 'Source Verified' and 'Verified' to a single 'Verified' category
combined_data = combined_data %>%
  mutate(verification_status = case_when(
    verification_status == 'Source Verified' ~ 'Verified',
    TRUE ~ verification_status
  ))
```

Before saving the file, we need to remove texts that will cause our models to fail.
Also remove loan status, it is perfectly collinear to loan_status_bin
```{r remove-LC-grade-and-descriptive-columns}
# As the loan grade is assigned by and exclusive to LC, it can introduce bias in this dataset
combined_data = subset(combined_data, select = -c(grade, sub_grade, desc, title, emp_title, zip_code, loan_status))
```

Final cleaning of some data
```{r}
combined_data <- combined_data %>%
  mutate(
    # Convert earliest_cr_line to a date format
    earliest_cr_line = paste0("01-", earliest_cr_line), 
    earliest_cr_line = dmy(earliest_cr_line),
    
    # Convert last_credit_pull_d to a date format
    last_credit_pull_d = paste0("01-", last_credit_pull_d),
    last_credit_pull_d = dmy(last_credit_pull_d),
    
    next_pymnt_d = paste0("01-", next_pymnt_d),  
    next_pymnt_d = dmy(next_pymnt_d) 

  )

```

```{r save-cleaned-data-as-rds}
# Save combined_data as a CSV file
write_csv(combined_data, "../data/combined_data.csv")
saveRDS(combined_data, "../data/combined_data.rds")
file.remove("../data/combined_data.csv")
```
## Index term to 0 or 1 -> To do before ML part with encoder (e.g. one-hot encoder)

# Once all is done
## 70/30 Data split
## Save CSV file as combined_data_cleaned.csv

# Future plans
## EDA
## Scale the data for the relevant regressions
## If the output is a probability, then >0.5 = bad loan, else good loan

# Models to be use
## 1. Benchmark model = Decision Tree + Logit
## 2. Random Forest
## 3. XGBoost
## 4. Ridge Classifier


