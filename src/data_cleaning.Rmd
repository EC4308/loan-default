---
title: "EC4308 Project (Lending Club)"
author: "Brandon, LX, WT, YH, ZH"
date: "2024-10-11"
output: html_document
---

### Initial setup
```{r setup, include=FALSE}
library(readr)
library(tidyverse)
library(jsonlite)
library(lubridate)
library(zoo)
library(ggcorrplot)
library(corrplot)
```

### Convert all the csv files to RDS
Very crucial step. Original file size was around 200 MB and 400 MB each. This meant we needed to use GIT LFS, which was unfeasable as we did not have funding to do so. We determined that RDS was a much more efficient way to save the data. 
```{r save-rds}
# Step 1: Load the CSV files
#lc_loan <- read_csv("../data/lc_loan.csv")
#lc_2016_2017 <- read_csv("../data/lc_2016_2017.csv")

# Step 2: Save the loaded data as RDS files
#saveRDS(lc_loan, "../data/lc_loan.rds")
#saveRDS(lc_2016_2017, "../data/lc_2016_2017.rds")
```
After saving the data as RDS, we noticed that each data was less than 100 MB, meaning that we did not need to use GIT LFS

### Read the RDS Files
```{r read-rds}
lc_loan <- readRDS("../data/lc_loan.rds")
lc_2016_2017 <- readRDS("../data/lc_2016_2017.rds")
```

### Analysing the head of the data
```{r head}
head(lc_loan)
```

## Data Cleaning
### Observe the difference in columns
```{r}
# Get the column names from both dataframes
lc_loan_cols <- colnames(lc_loan)
lc_2016_2017_cols <- colnames(lc_2016_2017)

# Compare the columns to find the differences
diff_cols <- setdiff(lc_loan_cols, lc_2016_2017_cols)
diff_cols_2 <- setdiff(lc_2016_2017_cols, lc_loan_cols)

# Display the differences
cat("Columns in 'lc_loan' but not in 'lc_2016_2017':", diff_cols, "\n")
cat("Columns in 'lc_2016_2017' but not in 'lc_loan':", diff_cols_2, "\n")
```

### Remove columns that are not common between the two dataframes
So there will be a clean horizontal join between all the data points. A sign of a good horizontal join will be that there is no _x and _y suffix.
```{r echo=FALSE}
lc_loan_clean <- lc_loan %>% select(-all_of(diff_cols))
lc_2016_2017_clean <- lc_2016_2017 %>% select(-all_of(diff_cols_2))
combined_data <- bind_rows(lc_loan_clean, lc_2016_2017_clean)
#glimpse(combined_data)
```
In the join, there is no _x and _y, meaning that it was a clean join.

### Convert loan_status to a factor (categorical variable)
Helps with the data analytics
```{r}
combined_data <- combined_data %>%
  mutate(loan_status = as.factor(loan_status))
table(combined_data$loan_status)
```

### Print out the possible levels of loan_status
```{r}
cat("Possible loan_status categories:\n")
print(levels(combined_data$loan_status))
```

### Remove "Current" as it adds no value to our analytics
```{r}
combined_data <- combined_data %>%
  filter(loan_status != "Current")
```

### Now convert it to binary
This will be our Y variable, whereby 0 denotes a good loan, and 1 denotes a bad loan.
```{r}
combined_data <- combined_data %>%
  mutate(
    will_default = case_when(
      loan_status %in% c("Fully Paid", "Does not meet the credit policy. Status:Fully Paid") ~ 0,
      TRUE ~ 1  # Everything else will be 1
    )
  )
```

### Check how many 0s and 1s in the new binary column
```{r}
loan_status_distribution <- table(combined_data$will_default)
print(loan_status_distribution)

paid_off_percentage <- (loan_status_distribution[1] / sum(loan_status_distribution)) * 100

cat("Percentage of loans that are paid off in the full data set:", round(paid_off_percentage, 2), "%\n")

```
## Reformatting the variables
### Employment length
Convert employment length to an ordered categorical data
```{r recode-employment-length}
# Encode "< 1 year" and missing values as 0, "10+ years" as ">10"
combined_data <- combined_data %>%
  mutate(emp_length = case_when(
    emp_length == '< 1 year' ~ "0",
    emp_length == 'n/a' ~ "0",
    is.na(emp_length) ~ "0",
    emp_length == '10+ years' ~ ">=10",  # Encode 10+ years as ">10"
    TRUE ~ gsub(" years?$", "", emp_length)  # Remove "year" or "years" from other values
  ))

# Convert emp_length to an ordered factor, ensuring ">10" is placed last
combined_data <- combined_data %>%
  mutate(emp_length = factor(emp_length, levels = c("0", "1", "2", "3", "4", "5", "6", "7", "8", "9", ">=10")))
```

### Verification status
Make verification status as boolean data, with 1 being verified and 0 being otherwise.
```{r recode-verification-status}
# Combine 'Source Verified' and 'Verified' to a single 'Verified' category
combined_data = combined_data %>%
  mutate(verification_status = case_when(
    verification_status == 'Source Verified' ~ 'Verified',
    TRUE ~ verification_status
  ))
```

### Converting to date
Covert some string form data to datetime format
```{r}
combined_data <- combined_data %>%
  mutate(
    earliest_cr_line = my(earliest_cr_line),
    last_credit_pull_d = my(last_credit_pull_d),
    next_pymnt_d = my(next_pymnt_d),
    last_pymnt_d = my(last_pymnt_d),
    issue_d = my(issue_d) 
  )
```
But one thing we know is that models only take in either continuous or categorical data. Meaning that string and datetime data formats will not be handle correctly.

So now we convert the data to numeric values, such as year and months.
```{r}
combined_data <- combined_data %>%
  mutate(
    earliest_cr_year = year(earliest_cr_line),
    earliest_cr_month = month(earliest_cr_line),

    last_credit_pull_year = year(last_credit_pull_d),
    last_credit_pull_month = month(last_credit_pull_d),

    next_pymnt_year = year(next_pymnt_d),
    next_pymnt_month = month(next_pymnt_d),
    
    last_pymnt_year = year(last_pymnt_d),
    last_pymnt_month = month(last_pymnt_d),
    
    issue_d_year = year(issue_d),
    issue_d_month = month(issue_d)
  )
```

### FRED data on Delinquency Rate on Consumer Loans, All Commercial Banks
```{r}
combined_data %>%
  mutate(quarter = as.yearqtr(ymd(issue_d))) -> combined_data

delinquency_rate <- read_csv("../data/DRCLACBS.csv") 
delinquency_rate <- delinquency_rate %>%
  rename("delinquency_rate" = "DRCLACBS") %>%
  mutate(DATE = as.yearqtr(ymd(DATE)), lag_quarter = lag(DATE), lag_delinquency = lag(delinquency_rate))
# We used lagged delinquency rate as it is likely at the point of issuance, one is not able to know the delinquency_rate for that period

combined_data %>%
  left_join(delinquency_rate, by = c("quarter" = "DATE")) -> combined_data
```

### FRED data on US 36-Month and 60-Month Treasury Yield to calculate the spread of interest rate on the consumer loan over the treasury yield in order to isolate changes in interest rate offered to consumers to the credit worthiness of a loan. There are only two durations of loans being offered: 36 months and 60 months.
```{r}
treasury_36mths <- read_csv("../data/DGS3.csv") 
treasury_60mths <- read_csv("../data/DGS5.csv") 

combined_data = combined_data %>%
  left_join(treasury_36mths %>% rename(treasury_yield_36 = DGS3) %>% mutate(treasury_yield_36 = as.numeric(treasury_yield_36)), by = c("issue_d" = "DATE")) %>%
  left_join(treasury_60mths %>% rename(treasury_yield_60 = DGS5), by = c("issue_d" = "DATE")) %>%
  mutate(
    treasury_yield = case_when(
      str_detect(term, "36") ~ treasury_yield_36,
      str_detect(term, "60") ~ treasury_yield_60,
      TRUE ~ NA_real_  # NA for any term that doesn't match "36" or "60"
    )
  ) %>%
  select(-treasury_yield_36, -treasury_yield_60)  %>% # Clean up intermediary columns if desired
  mutate(spread = int_rate - treasury_yield)
```

### Lending Club Issued Loans (Benchmark)
In this code chunk, we discover how successful Lending Club's internal ratings is at predicting default.
We make the assumption that Lending Club predicts that the loan will not be defaulted when a rating of A to C is given, while a loan will be defaulted when a rating of D to G is given. From this, we can build a confusion matrix to find the recall and F1 score of their current internal metric.
```{r}
# Group by 'sub_grade' and count the number of defaults
default_counts = combined_data %>%
  select(sub_grade,will_default) %>%
  group_by(sub_grade, will_default) %>%
  summarize(count = n(), .groups = "drop") %>% 
  pivot_wider(names_from = will_default, values_from = count, values_fill = 0) %>%
  rename(defaulters = `1`, non_defaulters = `0`) %>%
  mutate(
    predicted_default = ifelse(grepl("^[A-C]", sub_grade), "predicted_non_default", "predicted_default")
  )

confusion_matrix <- default_counts %>%
  group_by(predicted_default) %>%
  summarize(
    TP = sum(ifelse(predicted_default == "predicted_default", defaulters, 0)),  # True Positives
    FP = sum(ifelse(predicted_default == "predicted_default", non_defaulters, 0)),  # False Positives
    TN = sum(ifelse(predicted_default == "predicted_non_default", non_defaulters, 0)),  # True Negatives
    FN = sum(ifelse(predicted_default == "predicted_non_default", defaulters, 0))   # False Negatives
  ) %>%
  summarize(
    TP = sum(TP),
    FP = sum(FP),
    TN = sum(TN),
    FN = sum(FN)
  )

# Calculate recall and F1 score
recall <- confusion_matrix$TP / (confusion_matrix$TP + confusion_matrix$FN)
precision <- confusion_matrix$TP / (confusion_matrix$TP + confusion_matrix$FP)
F1_score <- 2 * (precision * recall) / (precision + recall)

# Display the confusion matrix
confusion_matrix_df <- data.frame(
  Prediction = c("Predicted Default", "Predicted Non-Default"),
  Actual_Default = c(confusion_matrix$TP, confusion_matrix$FN),
  Actual_Non_Default = c(confusion_matrix$FP, confusion_matrix$TN)
)

print(confusion_matrix_df)

# Output results
list(recall = recall, F1_score = F1_score)
```


### Stratify the data based on the loan_status and get ~ 50,000 accordingly
Random suggestion, compare the direct pulling of 50k data points vs stratified
```{r}
## Stratify data based on loan status
# Set seed for reproducibility
set.seed(42)

# Stratified Sampling by 'loan status'
strat_cols <- c("loan_status")

# Perform the stratified sampling for training set (20%)
train_data <- combined_data %>%
  group_by(across(all_of(strat_cols))) %>%
  sample_frac(0.2) %>% # Adjust the fraction as needed for training set
  ungroup()

# # Remaining data for test sets
test_data <- anti_join(combined_data, train_data)
```

### Random pulling
```{r}
# Set the seed for reproducibility
set.seed(4308)

# Randomly sample 50,000 rows from the dataset, as it is getting way too large
random_sampled_data <- combined_data %>%
  sample_n(size = 50000)

loan_status_distribution <- table(random_sampled_data$will_default)
print(loan_status_distribution)

paid_off_percentage <- (loan_status_distribution[1] / sum(loan_status_distribution)) * 100

cat("Percentage of loans that are paid off, when we do a random sample of 50,000:", round(paid_off_percentage, 2), "%\n")
```
Quite accurate as compared to the real world proportions. But by doing a random sample, we might not have enough readings for 1 as 30% is quite a small amount.

### Stratified
Maybe we can do a 50/50 stratification to get a better sensing on real data?
```{r}
## Random 50/50 stratification 
set.seed(123) 

combined_data <- combined_data %>%
  group_by(will_default) %>%
  sample_n(size = 25000, replace = TRUE)  # 50% from each class, adjust size as needed

# Check distribution
table(combined_data$will_default)
```

## EDA
### Employment Length
```{r}
combined_data %>%
  ggplot(aes(x=emp_length)) +
  geom_bar(fill='steelblue') +
  theme_minimal() + 
  labs(x='Employment Length', y='Number of Loans', title='Distribution of Employment Length (Cleaned)')

emp_length_loan_counts <- combined_data %>%
  group_by(emp_length, will_default) %>%
  summarize(loan_count = n()) %>%
  ungroup() %>%
  group_by(emp_length) %>%
  mutate(total_loans = sum(loan_count),
         percentage_loans = (loan_count / total_loans) * 100)

# Plot the stacked bar chart
emp_length_loan_counts %>%
  ggplot(aes(x = factor(emp_length), y = percentage_loans, fill = as.factor(will_default))) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  labs(
    x = 'Employment Length',
    y = 'Percentage of Loans',
    title = 'Distribution of Employment Length (Stacked by Loan Status)',
    fill = 'Loan Status'
  ) +
  scale_y_continuous(labels = scales::percent_format(scale = 1)) +
  scale_fill_manual(values = c("green", "red"), labels = c("Good Loan (0)", "Bad Loan (1)")) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```
We can see that most of the borrowers have been employed for 10 or more years. 

### Loan Amount
```{r}
combined_data %>%
  ggplot(aes(x = loan_amnt, fill = as.factor(will_default))) +
  geom_density(alpha = 0.4) +  # Transparency for overlapping densities
  labs(
    title = "Density Plot of Loan Amount by Loan Status",
    x = "Loan Amount",
    y = "Density",
    fill = "Loan Status"
  ) +
  scale_fill_manual(values = c("green", "red"), labels = c("Good Loan (0)", "Bad Loan (1)")) +
  theme_minimal() +
  theme(legend.position = "top")  # Moving legend to the top

```

### Percentage of loans offered over time
```{r}
# First, group by issue_d_year and will_default, and count the number of loans per group
loan_counts_per_year <- combined_data %>%
  group_by(issue_d_year, will_default) %>%
  summarize(loan_count = n()) %>%
  ungroup() %>%
  group_by(issue_d_year) %>%
  mutate(total_loans = sum(loan_count),
         percentage_loans = (loan_count / total_loans) * 100)

# Plot the stacked bar chart
loan_counts_per_year %>%
  ggplot(aes(x = issue_d_year, y = percentage_loans, fill = as.factor(will_default))) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  labs(
    title = "Percentage of Loans Offered Over Time by Year (Stacked by Loan Status)",
    x = "Year of Issue",
    y = "Percentage of Total Loans",
    fill = "Loan Status"
  ) +
  scale_y_continuous(labels = scales::percent_format(scale = 1)) +
  scale_fill_manual(values = c("green", "red"), labels = c("Good Loan (0)", "Bad Loan (1)")) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

### Plotting subgrade
```{r}
sub_grade_distribution <- combined_data %>%
  group_by(sub_grade) %>%
  summarise(count = n()) %>%
  mutate(proportion = count / sum(count))

# Plot the distribution with sub_grade ordered by factor level (alphabetically)
ggplot(sub_grade_distribution, aes(x = sub_grade, y = proportion)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  theme_minimal() +
  labs(title = "Distribution of Sub Grade",
       x = "Sub Grade",
       y = "Proportion") +
  scale_y_continuous(labels = scales::percent_format()) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

### Plotting grade vs loan status
```{r}
grade_distribution <- combined_data %>%
  group_by(grade, will_default) %>%
  summarise(loan_count = n()) %>%
  ungroup() %>%
  group_by(grade) %>%
  mutate(total_loans = sum(loan_count),
         percent_loans = (loan_count / total_loans) * 100)

# Plot the distribution of grade
combined_data %>%
  ggplot(aes(x=grade)) +
  geom_bar(fill='steelblue') +
  theme_minimal() + 
  labs(x='Loan Grade', y='Number of Loans', title='Distribution of Loan Grade')

# Plot the distribution of grade versus loan status
ggplot(grade_distribution, aes(x = grade, y = percent_loans, fill = as.factor(will_default))) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  labs(title = "Distribution of Grade",
       x = "Grade",
       y = "Proportion",
       fill = "Loan Status"
  ) +
  scale_y_continuous(labels = scales::percent_format(scale = 1)) +
  scale_fill_manual(values = c("green", "red"), labels = c("Good Loan (0)", "Bad Loan (1)")) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
Most of the loans typically fall within the B-C grade range, and as expected, we see that the percent of bad loans climb steadily as the loan grade worsens.

### Remove descriptive text
Before saving the file, we need to remove texts that will cause our models to fail. Also remove loan status, it is perfectly collinear to will_default
```{r remove-LC-grade-and-descriptive-columns}
original_columns <- colnames(combined_data)

combined_data <- combined_data %>%
  select(
    loan_amnt, term, spread, installment, dti, pymnt_plan, 
    emp_length, home_ownership, annual_inc, verification_status,
    purpose, mths_since_last_delinq, mths_since_last_record, delinq_2yrs, 
    inq_last_6mths, open_acc, pub_rec, revol_bal, revol_util, total_acc,   
    will_default, lag_delinquency, addr_state, term, application_type
  )

selected_columns <- colnames(combined_data)
removed_columns <- setdiff(original_columns, selected_columns)

# Display the removed columns
cat("Columns removed:\n", paste(removed_columns, collapse = ", "), "\n")
```

### Categorical data
Some of the data are still strings instead of categorical data, we can use the `as.factor` function to make them categorical
```{r}
combined_data <- combined_data %>%
  mutate(home_ownership = as.factor(home_ownership),
         verification_status = as.factor(verification_status),
         pymnt_plan = as.factor(pymnt_plan),
         term = as.factor(term),
         addr_state = as.factor(addr_state),
         application_type = as.factor(application_type),
         will_default = as.factor(will_default)
         )

combined_data <- combined_data %>%
  mutate(application_type = recode(application_type,
                                   "Individual" = "Individual",
                                   "INDIVIDUAL" = "Individual",
                                   "Joint App" = "Joint",
                                   "JOINT" = "Joint"))


```

### Check the levels
```{r}
factor_columns <- combined_data %>% select(where(is.factor))

lapply(factor_columns, levels)
```

### Final check
```{r}
str(combined_data)
```

## Save the data as a RDS file
```{r save-cleaned-data-as-rds}
# Save combined_data as a CSV file
saveRDS(combined_data, "../data/combined_data.rds")
```

